[{"authors":["admin"],"categories":null,"content":"Hello and welcome! My Vietnamese name is Nguyễn Trung Tín. I therefore used \u0026ldquo;TrungTin Nguyen\u0026rdquo; or \u0026ldquo;Trung Tin Nguyen\u0026rdquo; in my English publications. The first name is also \u0026ldquo;Tín\u0026rdquo; or \u0026ldquo;Tin\u0026rdquo; for short.\nI am currently a Postdoctoral Research Fellow at the The University of Queensland in the School of Mathematics and Physics from December 2023, where I am very fortunate to be mentored by Hien Duy Nguyen, and Xin Guo.\nBefore going to Queensland, I was a Postdoctoral Research Fellow at the Inria centre at the University Grenoble Alpes in the Statify team, where I was very fortunate to be mentored by Florence Forbes, Julyan Arbel, and collaborated with Hien Duy Nguyen as part of an international project team WOMBAT.\nI completed my Ph.D. Degree in Statistics and Data Science at Normandie Univ in December 2021, where I was very fortunate to have been advised by Faicel Chamroukhi. During my Ph.D. research, I am grateful to collaborate with Hien Duy Nguyen, and Geoff McLachlan. I received a Visiting PhD Fellowship for 4 months at the Inria centre at the University Grenoble Alpes in the Statify team within a project LANDER.\nA central theme of my research is data science, at the intersection of:\nStatistical learning: Model selection (minimal penalties and slope heuristics, non-asymptotic oracle inequalities), simulation-based inference (approximate Bayesian computation, Bayesian synthetic likelihood, method of moments), Bayesian nonparametrics (Gibbs-type priors, Dirichlet process mixture), high-dimensional statistics (variable selection via Lasso and penalization, graphical models), uncertainty estimation. Machine learning: Supervised learning (deep hierarchical mixture of experts (DMoE), deep neural networks), unsupervised learning (clustering via mixture models, dimensionality reduction via principal component analysis, deep generative models via variational autoencoders, generative adversarial networks and normalizing flows), reinforcement learning (partially observable Markov decision process). Optimization: Robust and effective optimization algorithms for mixture models (expectation–maximization, variational Bayesian expectation–maximization, Markov chain Monte Carlo methods), difference of convex algorithm, optimal transport (Wasserstein distance, voronoi loss function). Applications: Natural language processing (large language model), remote sensing (planetary science, e.g., retrieval of Mars surface physical properties from hyper-spectral images), signal processing (sound source localization), biostatistics (genomics, transcriptomics, proteomics), computer vision (image segmentation), quantum chemistry, drug discovery, and materials science (supervised and unsupervised learning on molecular modeling). I have been very fortunate to have had fruitful collaborations since my PhD with Nhat Ho, Huy Nguyen, Khai Nguyen, Quang Pham, Binh Nguyen, Giang Truong Do, Le Huy Khiem, Dung Ngoc Nguyen, and Ho Minh Duy Nguyen (Collabolators in random order).\n","date":1698883200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1698883200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/trungtin-nguyen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/trungtin-nguyen/","section":"authors","summary":"Hello and welcome! My Vietnamese name is Nguyễn Trung Tín. I therefore used \u0026ldquo;TrungTin Nguyen\u0026rdquo; or \u0026ldquo;Trung Tin Nguyen\u0026rdquo; in my English publications. The first name is also \u0026ldquo;Tín\u0026rdquo; or \u0026ldquo;Tin\u0026rdquo; for short.","tags":null,"title":"TrungTin Nguyen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"49cbbfcf1124f111a37fcbac0faeb48e","permalink":"/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"page"},{"authors":["Huy Nguyen","TrungTin Nguyen","Nhat Ho"],"categories":null,"content":"","date":1698883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1698883200,"objectID":"f314b6eee7609982b10a606e8d007446","permalink":"/publication/nguyen2023demystifying/","publishdate":"2023-11-02T00:00:00Z","relpermalink":"/publication/nguyen2023demystifying/","section":"publication","summary":"Understanding the parameter estimation of softmax gating Gaussian mixture of experts has remained a long-standing open problem in the literature. It is mainly due to three fundamental theoretical challenges associated with the softmax gating function$:$ (i) the identifiability only up to the translation of parameters; (ii) the intrinsic interaction via partial differential equations between the softmax gating and the expert functions in the Gaussian density; (iii) the complex dependence between the numerator and denominator of the conditional density of softmax gating Gaussian mixture of experts. We resolve these challenges by proposing novel Voronoi loss functions among parameters and establishing the convergence rates of maximum likelihood estimator (MLE) for solving parameter estimation in these models. When the true number of experts is unknown and over-specified, our findings show a connection between the convergence rate of the MLE and a solvability problem of a system of polynomial equations.","tags":null,"title":"Demystifying Softmax Gating Function in Gaussian Mixture of Experts","type":"publication"},{"authors":["Huy Nguyen","Pedram Akbarian","TrungTin Nguyen","Nhat Ho"],"categories":null,"content":"","date":1697932800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697932800,"objectID":"abe8416f9c47334ec3457393bcb32a4c","permalink":"/publication/nguyen2023general/","publishdate":"2023-10-22T00:00:00Z","relpermalink":"/publication/nguyen2023general/","section":"publication","summary":"Mixture-of-experts (MoE) model incorporates the power of multiple submodels via gating functions to achieve greater performance in numerous regression and classification applications. From a theoretical perspective, while there have been previous attempts to comprehend the behavior of that model under the regression settings through the convergence analysis of maximum likelihood estimation in the Gaussian MoE model, such analysis under the setting of a classification problem has remained missing in the literature. We close this gap by establishing the convergence rates of density estimation and parameter estimation in the softmax gating multinomial logistic MoE model. Notably, when part of the expert parameters vanish, these rates are shown to be slower than polynomial rates owing to an inherent interaction between the softmax gating and expert functions via partial differential equations. To address this issue, we propose using a novel class of modified softmax gating functions which transform the input value before delivering them to the gating functions. As a result, the previous interaction disappears and the parameter estimation rates are significantly improved.","tags":null,"title":"A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts","type":"publication"},{"authors":["Truong Giang Do","Huy Khiem Le","Quang Pham","TrungTin Nguyen","Binh T. Nguyen","Thanh-Nam Doan","Chenghao Liu","Savitha Ramasamy","Xiaoli Li","Steven HOI"],"categories":null,"content":"","date":1694476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"9de7d0674812d72e45f009e6302de589","permalink":"/publication/do2023hyperouter/","publishdate":"2023-09-12T00:00:00Z","relpermalink":"/publication/do2023hyperouter/","section":"publication","summary":"By routing input tokens to only a few split experts, Sparse Mixture-of-Experts has enabled efficient training of large language models. Recent findings suggest that fixing the routers can achieve competitive performance by alleviating the collapsing problem, where all experts eventually learn similar representations. However, this strategy has two key limitations$:$ (i) the policy derived from random routers might be sub-optimal, and (ii) it requires a substantial number of experts during evaluation, leading to limited efficiency gains during inference. This work introduces HyperRouter, which dynamically generates the router's parameters through a fixed hypernetwork and trainable embeddings. Consequently, HyperRouter achieves a balance between training the routers and freezing them to learn an improved routing policy. Extensive experiments across a wide range of tasks demonstrate the superior performance and efficiency gains of HyperRouter compared to existing routing methods. Our implementation will be made available upon acceptance.","tags":null,"title":"HyperRouter: Towards Efficient Training and Inference of Sparse Mixture of Experts","type":"publication"},{"authors":["Huy Nguyen","TrungTin Nguyen","Khai Nguyen","Nhat Ho"],"categories":null,"content":"","date":1683849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683849600,"objectID":"11999abd39f65abcd45325ae572e7122","permalink":"/publication/nguyen2023towards/","publishdate":"2023-05-12T00:00:00Z","relpermalink":"/publication/nguyen2023towards/","section":"publication","summary":"Originally introduced as a neural network for ensemble learning, mixture of experts (MoE) has recently become a fundamental building block of highly successful modern deep neural networks for heterogeneous data analysis in several applications, including those in machine learning, statistics, bioinformatics, economics, and medicine. Despite its popularity in practice, a satisfactory level of understanding of the convergence behavior of Gaussian-gated MoE parameter estimation is far from complete. The underlying reason for this challenge is the inclusion of covariates in the Gaussian gating and expert networks, which leads to their intrinsically complex interactions via partial differential equations with respect to their parameters. We address these issues by designing novel Voronoi loss functions to accurately capture heterogeneity in the maximum likelihood estimator (MLE) for resolving parameter estimation in these models. Our results reveal distinct behaviors of the MLE under two settings$:$ the first setting is when all the location parameters in the Gaussian gating are non-zeros while the second setting is when there exists at least one zero-valued location parameter. Notably, these behaviors can be characterized by the solvability of two different systems of polynomial equations. Finally, we conduct a simulation study to verify our theoretical results.","tags":null,"title":"Towards Convergence Rates for Parameter Estimation in Gaussian-gated Mixture of Experts","type":"publication"},{"authors":null,"categories":null,"content":"","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"95267f55ff5783e1fc765da2dffa116b","permalink":"/project/convergence-rates-for-mixtures-of-experts/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/project/convergence-rates-for-mixtures-of-experts/","section":"project","summary":"Originally introduced as a neural network for ensemble learning, mixture of experts (MoE) has recently become a fundamental building block of highly successful modern deep neural networks for heterogeneous data analysis in several applications, including those in machine learning, statistics, bioinformatics, economics, and medicine. Despite its popularity in practice, a satisfactory level of understanding of the convergence behavior of Gaussian-gated MoE parameter estimation is far from complete. The underlying reason for this challenge is the inclusion of covariates in the Gaussian gating and expert networks, which leads to their intrinsically complex interactions via partial differential equations with respect to their parameters. We address these issues by designing novel Voronoi loss functions to accurately capture heterogeneity in the maximum likelihood estimator (MLE) for resolving parameter estimation in these models. Our results reveal distinct behaviors of the MLE under two settings$:$ the first setting is when all the location parameters in the Gaussian gating are non-zeros while the second setting is when there exists at least one zero-valued location parameter. Notably, these behaviors can be characterized by the solvability of two different systems of polynomial equations. Finally, we conduct a simulation study to verify our theoretical results.","tags":["Convergence Rates for Mixtures of Experts"],"title":"Convergence Rates for Mixtures of Experts","type":"project"},{"authors":["TrungTin Nguyen","Florence Forbes","Julyan Arbel","Hien Duy Nguyen"],"categories":null,"content":"","date":1677974400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677974400,"objectID":"0f3ae85fb80429e915b3e05c7553856a","permalink":"/publication/nguyen2023bayesian/","publishdate":"2023-03-05T00:00:00Z","relpermalink":"/publication/nguyen2023bayesian/","section":"publication","summary":"A large class of problems can be formulated as inverse problems, where the goal is to find parameter values that best explain some observed measures. Typical constraints in practice are that the relationships between parameters and observations are highly nonlinear, with high-dimensional observations and multi-dimensionally correlated parameters. To deal with these constraints, we consider probabilistic mixtures of locally linear models using inverse regression strategies, namely the Gaussian locally linear mapping (GLLiM) models. These can be seen as special instances of a mixture of experts (MoE) models. The popularity of MoE is largely due to their universal approximation properties, provided that the number of mixture components is large enough. In this paper, we propose a general scheme to design a tractable Bayesian nonparametric GLLiM (BNP-GLLiM) model to avoid any commitment to an arbitrary number of components. A tractable estimation algorithm is designed using a variational Bayesian expectation-maximization. In particular, we establish posterior consistency for the number of components in BNP-GLLiM after the merge-truncate-merge algorithm post-processing. Illustrations on simulated data show good results in terms of recovering the true number of clusters and the mean regression function.","tags":null,"title":"Bayesian nonparametric mixture of experts for high-dimensional inverse problems","type":"publication"},{"authors":["Hien Duy Nguyen","TrungTin Nguyen","Julyan Arbel","Florence Forbes"],"categories":null,"content":"","date":1676332800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676332800,"objectID":"0ef886d8e9dd9e964d094ed632af0cc0","permalink":"/publication/nguyen2023concentration/","publishdate":"2023-02-14T00:00:00Z","relpermalink":"/publication/nguyen2023concentration/","section":"publication","summary":"We study the large sample behaviors of approximate Bayesian computation (ABC) posterior measures in situations when the data generating process is dependent on non-identifiable parameters. In particular, we establish the concentration of posterior measures on sets of arbitrarily measure that contain the equivalence set of the data generative parameter, when the sample size tends to infinity. Our theory also makes weak assumptions regarding the measurement of discrepancy between the data set and simulations, and in particular, does not require the use of summary statistics and is applicable to a broad class of kernelized ABC algorithms. We provide useful illustrations and demonstrations of our theory in practice, and offer a comprehensive assessment of the nature in which our findings complement other results in the literature.","tags":null,"title":"Concentration results for approximate Bayesian computation without identifiability","type":"publication"},{"authors":["TrungTin Nguyen","Dung Ngoc Nguyen","Hien Duy Nguyen","Faicel Chamroukhi"],"categories":null,"content":"","date":1676073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676073600,"objectID":"0d65cde0ff7a50b887ca28690ca3d43c","permalink":"/publication/nguyen2023nonasymptotic/","publishdate":"2023-02-11T00:00:00Z","relpermalink":"/publication/nguyen2023nonasymptotic/","section":"publication","summary":"We are motivated by the problem of identifying potentially nonlinear regression relationships between high-dimensional outputs and high-dimensional inputs of heterogeneous data. This requires regression, clustering, and model selection, simultaneously. In this framework, we apply the mixture of experts models which are among the most popular ensemble learning techniques developed in the field of neural networks. In particular, we consider a more general case of mixture of experts models characterized by multiple Gaussian experts whose means are polynomials of the input variables and whose covariance matrices have block-diagonal structures. More especially, each expert is weighted by a gating network that is a softmax function of a polynomial of the input variables. These models require several hyper-parameters, including the number of mixture components, the complexity of the softmax gating networks and Gaussian mean experts, and the hidden block-diagonal structures of the covariance matrices.  We provide a non-asymptotic theory for model selection of such complex hyper-parameters using the slope heuristic approach in a penalized maximum likelihood estimation framework. Specifically, we establish a non-asymptotic risk bound on the penalized maximum likelihood estimation, which takes the form of an oracle inequality, given lower bound assumptions on the penalty function.","tags":null,"title":"A non-asymptotic risk bound for model selection in high-dimensional mixture of experts via joint rank and variable selection","type":"publication"},{"authors":null,"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"078bc1dff736f24743a241f72dcb1724","permalink":"/project/deep-neural-networks/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/project/deep-neural-networks/","section":"project","summary":"","tags":["Deep neural networks"],"title":"Deep Neural Networks","type":"project"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1670940000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670940000,"objectID":"c6a89fef63bba04ce8355d3a68067856","permalink":"/talk/icsds2022/","publishdate":"2022-12-12T10:30:00Z","relpermalink":"/talk/icsds2022/","section":"talk","summary":"Mixture of experts (MoE) are a popular class of statistical and machine learning models that have gained attention over the years due to their flexibility and efficiency. In this work, we consider Gaussian-gated localized MoE (GLoME) and block-diagonal covariance localized MoE (BLoME) regression models to present nonlinear relationships in heterogeneous data with potential hidden graph-structured interactions between high-dimensional predictors. These models pose difficult statistical estimation and model selection questions, both from a computational and theoretical perspective. This paper is devoted to the study of the problem of model selection among a collection of GLoME or BLoME models characterized by the number of mixture components, the complexity of Gaussian mean experts, and the hidden block-diagonal structures of the covariance matrices, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds for the penalties hold. The good empirical behavior of our models is then demonstrated on synthetic and real datasets.","tags":[],"title":"A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts models","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1666620000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666620000,"objectID":"7a13ffa204c1a94587e0a2cf187c3504","permalink":"/talk/bnp132022/","publishdate":"2022-10-20T10:30:00Z","relpermalink":"/talk/bnp132022/","section":"talk","summary":"A wide class of problems can be formulated as inverse problems where the goal is to find parameter values that best explain some observed measures. Typical constraints in practice are that relationships between parameters and observations are highly non-linear, with high-dimensional observations and multi-dimensional correlated parameters.  To handle these constraints, we consider probabilistic mixtures of locally linear models, which can be seen as particular instances of mixtures of experts (MoE). We have shown in previous studies that such models had a good approximation ability provided the number of experts was large enough. This contribution is to propose a general scheme to design a tractable Bayesian nonparametric (BNP) MoE model to avoid any commitment to an arbitrary number of experts. A tractable estimation algorithm is designed using a variational approximation and theoretical properties are derived on the predictive distribution and the number of components. Illustrations on simulated and real data show good results in terms of selection and computing time compared to more traditional model selection procedures.","tags":[],"title":"Bayesian nonparametric mixture of experts for high-dimensional inverse problems","type":"talk"},{"authors":["Florence Forbes","Hien Duy Nguyen","TrungTin Nguyen","Julyan Arbel"],"categories":null,"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664582400,"objectID":"0c888954d53abbb8ee7d6c9d2e7b2ee2","permalink":"/publication/forbes2022summary/","publishdate":"2022-10-01T00:00:00Z","relpermalink":"/publication/forbes2022summary/","section":"publication","summary":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen and this can affect the approximations. Their choice is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics, to date. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated, a standard L2 distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to functional summary statistics. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.","tags":null,"title":"Summary statistics and discrepancy measures for approximate Bayesian computation via surrogate posteriors","type":"publication"},{"authors":["TrungTin Nguyen","Hien Duy Nguyen","Faicel Chamroukhi","Florence Forbes"],"categories":null,"content":"","date":1664323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664323200,"objectID":"9dfb50ae2006857815ada2093b37bd06","permalink":"/publication/nguyen2022nonasymptotic/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/publication/nguyen2022nonasymptotic/","section":"publication","summary":"Mixture of experts (MoE) are a popular class of statistical and machine learning models that have gained attention over the years due to their flexibility and efficiency. In this work, we consider Gaussian-gated localized MoE (GLoME) and block-diagonal covariance localized MoE (BLoME) regression models to present nonlinear relationships in heterogeneous data with potential hidden graph-structured interactions between high-dimensional predictors. These models pose difficult statistical estimation and model selection questions, both from a computational and theoretical perspective. This paper is devoted to the study of the problem of model selection among a collection of GLoME or BLoME models characterized by the number of mixture components, the complexity of Gaussian mean experts, and the hidden block-diagonal structures of the covariance matrices, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds for the penalties hold. The good empirical behavior of our models is then demonstrated on synthetic and real datasets.","tags":["Source Themes"],"title":"A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts","type":"publication"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1655128800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655128800,"objectID":"454098eee9f8bfe28bd34183aeefce8f","permalink":"/talk/jds2022/","publishdate":"2022-06-06T10:30:00Z","relpermalink":"/talk/jds2022/","section":"talk","summary":"This study is devoted to the problem of model selection among a collection of Gaussian-gated localized mixtures of experts models characterized by the number of mixture components, and the complexity of Gaussian mean experts, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds of the penalties hold. Their good empirical behavior is then demonstrated on synthetic and real datasets.","tags":[],"title":"Model selection by penalization in mixture of experts models with a non-asymptotic approach.","type":"talk"},{"authors":["Florence Forbes","Hien Duy Nguyen","TrungTin Nguyen","Julyan Arbel"],"categories":null,"content":"","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"2f3497ab28c44ca938f040b3433db9e2","permalink":"/publication/forbes2022mixture/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/publication/forbes2022mixture/","section":"publication","summary":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from expert or prior knowledge, which seldom occurs, they have to be chosen and this can affect the quality of approximations. The choice between discrepancies is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statis- tics. In this work, we introduce a preliminary learning step in which surrogate posteriors are built using a specific instance of a Mixture of Experts model. These surrogate pos- teriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. The resulting ABC quasi-posterior distribu- tion is shown to converge to the true one, under standard conditions. Experiments show that our approach is particularly useful when the posterior is multimodal.","tags":null,"title":"Mixture of expert posterior surrogates for approximate Bayesian computation","type":"publication"},{"authors":["TrungTin Nguyen","Faicel Chamroukhi","Hien Duy Nguyen","Florence Forbes"],"categories":null,"content":"","date":1655078400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655078400,"objectID":"bb56091cbacc6d03383a38959fe0977a","permalink":"/publication/nguyen2022model/","publishdate":"2022-06-13T00:00:00Z","relpermalink":"/publication/nguyen2022model/","section":"publication","summary":"This study is devoted to the problem of model selection among a collection of Gaussian-gated localized mixtures of experts models characterized by the number of mixture components, and the complexity of Gaussian mean experts, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds of the penalties hold. Their good empirical behavior is then demonstrated on synthetic and real datasets.","tags":null,"title":"Model selection by penalization in mixture of experts models with a non-asymptotic approach","type":"publication"},{"authors":["TrungTin Nguyen","Faicel Chamroukhi","Hien Duy Nguyen","Geoffrey J McLachlan"],"categories":null,"content":"","date":1653523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653523200,"objectID":"bfb5596881d966b42cb85ea8d4c3a9fe","permalink":"/publication/nguyen2020approximationlebesgue/","publishdate":"2022-05-26T00:00:00Z","relpermalink":"/publication/nguyen2020approximationlebesgue/","section":"publication","summary":"The class of location-scale finite mixtures is of enduring interest both from applied and theoretical perspectives of probability and statistics. We prove the following results; to an arbitrary degree of accuracy, (a) location-scale mixtures of a continuous probability density function (PDF) can approximate any continuous PDF, uniformly, on a compact set; and (b) for any finite $p \\in [1,\\infty)$, location-scale mixtures of an essentially bounded PDF can approximate any PDF in $L_p$, in the $L_p$ norm.","tags":["Source Themes"],"title":"Approximation of probability density functions via location-scale finite mixtures in Lebesgue spaces","type":"publication"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1652277600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652277600,"objectID":"015d6081cf367cca9e64e3f87d7bc4eb","permalink":"/talk/sasviasm2022/","publishdate":"2022-05-09T10:30:00Z","relpermalink":"/talk/sasviasm2022/","section":"talk","summary":"Mixture of experts (MoE) are a popular class of statistical and machine learning models that have gained attention over the years due to their flexibility and efficiency. In this work, we consider Gaussian-gated localized MoE (GLoME) and block-diagonal covariance localized MoE (BLoME) regression models to present nonlinear relationships in heterogeneous data with potential hidden graph-structured interactions between high-dimensional predictors. These models pose difficult statistical estimation and model selection questions, both from a computational and theoretical perspective. This paper is devoted to the study of the problem of model selection among a collection of GLoME or BLoME models characterized by the number of mixture components, the complexity of Gaussian mean experts, and the hidden block-diagonal structures of the covariance matrices, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds for the penalties hold. The good empirical behavior of our models is then demonstrated on synthetic and real datasets.","tags":[],"title":"A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts models.","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1649062800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649062800,"objectID":"94493c0fd1545e652f7138a22dc98775","permalink":"/talk/statlearn2022/","publishdate":"2022-03-15T10:30:00Z","relpermalink":"/talk/statlearn2022/","section":"talk","summary":"Mixture of experts (MoE), originally introduced as a neural network, is a popular class of statistical and machine learning models that has gained attention over the years due to its flexibility and efficiency. They provide conditional constructions for regression in which the mixture weights, along with the component densities, are explained by the predictors, allowing for flexibility in the modeling of data arising from complex data generating processes.  We have shown in previous studies that such models had a good approximation ability provided the number of experts was large enough. More precisely, to an arbitrary degree of accuracy, given input and output variables are both compactly supported, we have provided denseness results in Lebesgue spaces for conditional probability density functions. In this work, we consider Gaussian-gated localized MoE (GLoME) and block-diagonal covariance localized MoE (BLoME) regression models to present nonlinear relationships in heterogeneous data with potential hidden graph-structured interactions between high-dimensional predictors. These models pose difficult questions in statistical estimation and model selection, both from a computational and theoretical perspective. This talk is devoted to the study of the problem of model selection among a collection of GLoME or BLoME models characterized by the number of mixture components, the complexity of Gaussian mean experts, and the hidden block-diagonal structures of the covariance matrices, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds of the penalties hold. Their good empirical behavior is then demonstrated on synthetic and real datasets.","tags":[],"title":"A non-asymptotic approach for model selection via penalization in mixture of experts models","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1647601200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647601200,"objectID":"50ad14bf10eb90b0ffe15608ce95c6ff","permalink":"/talk/ensai2022/","publishdate":"2022-03-15T10:30:00Z","relpermalink":"/talk/ensai2022/","section":"talk","summary":"Mixture of experts (MoE), originally introduced as a neural network, is a popular class of statistical and machine learning models that has gained attention over the years due to its flexibility and efficiency. They provide conditional constructions for regression in which the mixture weights, along with the component densities, are explained by the predictors, allowing for flexibility in the modeling of data arising from complex data generating processes.  We have shown in previous studies that such models had a good approximation ability provided the number of experts was large enough. More precisely, to an arbitrary degree of accuracy, given input and output variables are both compactly supported, we have provided denseness results in Lebesgue spaces for conditional probability density functions. In this work, we consider Gaussian-gated localized MoE (GLoME) and block-diagonal covariance localized MoE (BLoME) regression models to present nonlinear relationships in heterogeneous data with potential hidden graph-structured interactions between high-dimensional predictors. These models pose difficult questions in statistical estimation and model selection, both from a computational and theoretical perspective. This talk is devoted to the study of the problem of model selection among a collection of GLoME or BLoME models characterized by the number of mixture components, the complexity of Gaussian mean experts, and the hidden block-diagonal structures of the covariance matrices, in a penalized maximum likelihood estimation framework. In particular, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds of the penalties hold. Their good empirical behavior is then demonstrated on synthetic and real datasets.","tags":[],"title":"A non-asymptotic model selection in mixture of experts models","type":"talk"},{"authors":null,"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"94ea898e1fce68f5fb93ec461a0840ff","permalink":"/project/bayesian-nonparametrics-in-mixture-of-experts-models/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/project/bayesian-nonparametrics-in-mixture-of-experts-models/","section":"project","summary":"A wide class of problems can be formulated as inverse problems where the goal is to find parameter values that best explain some observed measures. Typical constraints in practice are that relationships between parameters and observations are highly non-linear, with high-dimensional observations and multi-dimensional correlated parameters.  To handle these constraints, we consider probabilistic mixtures of locally linear models, which can be seen as particular instances of mixtures of experts (MoE). We have shown in previous studies that such models had a good approximation ability provided the number of experts was large enough. This contribution is to propose a general scheme to design a tractable Bayesian nonparametric (BNP) MoE model to avoid any commitment to an arbitrary number of experts. A tractable estimation algorithm is designed using a variational approximation and theoretical properties are derived on the predictive distribution and the number of components. Illustrations on simulated and real data show good results in terms of selection and computing time compared to more traditional model selection procedures.","tags":["Bayesian nonparametrics in mixture of experts models"],"title":"Bayesian Nonparametrics in Mixture of Experts Models","type":"project"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1639490400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639490400,"objectID":"f05d148b9d20734ffd8ebf3da80daf23","permalink":"/publication/nguyen2021modelthesis/","publishdate":"2021-12-14T14:00:00Z","relpermalink":"/publication/nguyen2021modelthesis/","section":"publication","summary":"Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, bioinformatics, pattern recognition, economics, and medicine, among many others. They provide conditional constructions for regression in which the mixture weights, along with the component densities, are explained by the predictors, allowing for flexibility in the modeling of data arising from complex data generating processes. In this thesis, we study the approximation capabilities and model estimation and selection properties, of a wide variety of mixture distributions, with a particular focus on a rich family of MoE models in a high-dimensional setting, including MoE models with Gaussian experts and softmax or Gaussian gating functions, which are the most popular choices and are powerful tools for modeling complex non-linear relationships between responses and predictors that arise from different subpopulations. We consider both the theoretical statistical and methodological aspects, and the numerical tools, related to the conception of these models, as well as to their data-driven estimation and model selection. More precisely, in this thesis, we first review the universal approximation properties of classical mixture distributions in order to prepare the theoretical framework and to clarify some unclear and vague statements in the literature, before considering them in the context of MoE models. In particular, we prove that, to an arbitrary degree of accuracy, location-scale mixtures of a continuous probability density function (PDF) can approximate any continuous PDF, uniformly, on a compact set; and location-scale mixtures of an essentially-bounded PDF can approximate any PDF in Lebesgue spaces. Then, after improving upon approximation results in the context of unconditional mixture distributions, we study the universal approximation capabilities of MoE models in a variety of contexts, including conditional density approximation and approximate Bayesian computation (ABC). Given input and output variables are both compactly supported, we provide denseness results in Lebesgue spaces for conditional PDFs. Moreover, we prove that the quasi-posterior distribution resulting from ABC with surrogate posteriors built from finite Gaussian mixtures using an inverse regression approach, converges to the true one, under standard conditions. Finally, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds on the penalties hold true, in high-dimensional regression scenarios for a variety of MoE regression models, including Gaussian-gated and softmax-gated Gaussian MoE, based on an inverse regression strategy or a Lasso penalization, respectively. In particular, our oracle inequalities show that the performance in Jensen–Kullback–Leibler type loss of our penalized maximum likelihood estimators are roughly comparable to that of oracle models if we take large enough the constants in front of the penalties, whose forms are only known up to multiplicative constants and proportional to the dimensions of models. Such theoretical justifications of the penalty shapes motivate us to make use of the slope heuristic criterion to select several hyperparameters, including the number of mixture components, the amount of sparsity (the coefficients and ranks sparsity levels), the degree of polynomial mean functions, and the potential hidden block-diagonal structures of the covariance matrices of the multivariate predictor or response variable. To support our theoretical results and the statistical study of non-asymptotic model selection in a variety of MoE models, we perform numerical studies by considering simulated and real data, which highlight the performance of our finite-sample oracle inequality results.","tags":null,"title":"Model Selection and Approximation in High-dimensional Mixtures of Experts Models: from Theory to Practice","type":"publication"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1639488600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639488600,"objectID":"f3fcb1b3f167364caa510ac00df0126c","permalink":"/talk/soutenancedethese2021decembre14/","publishdate":"2021-12-01T12:01:00Z","relpermalink":"/talk/soutenancedethese2021decembre14/","section":"talk","summary":"Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, bioinformatics, pattern recognition, economics, and medicine, among many others. They provide conditional constructions for regression in which the mixture weights, along with the component densities, are explained by the predictors, allowing for flexibility in the modeling of data arising from complex data generating processes. In this thesis, we study the approximation capabilities and model estimation and selection properties, of a wide variety of mixture distributions, with a particular focus on a rich family of MoE models in a high-dimensional setting, including MoE models with Gaussian experts and softmax or Gaussian gating functions, which are the most popular choices and are powerful tools for modeling complex non-linear relationships between responses and predictors that arise from different subpopulations. We consider both the theoretical statistical and methodological aspects, and the numerical tools, related to the conception of these models, as well as to their data-driven estimation and model selection. More precisely, in this thesis, we first review the universal approximation properties of classical mixture distributions in order to prepare the theoretical framework and to clarify some unclear and vague statements in the literature, before considering them in the context of MoE models. In particular, we prove that, to an arbitrary degree of accuracy, location-scale mixtures of a continuous probability density function (PDF) can approximate any continuous PDF, uniformly, on a compact set; and location-scale mixtures of an essentially-bounded PDF can approximate any PDF in Lebesgue spaces. Then, after improving upon approximation results in the context of unconditional mixture distributions, we study the universal approximation capabilities of MoE models in a variety of contexts, including conditional density approximation and approximate Bayesian computation (ABC). Given input and output variables are both compactly supported, we provide denseness results in Lebesgue spaces for conditional PDFs. Moreover, we prove that the quasi-posterior distribution resulting from ABC with surrogate posteriors built from finite Gaussian mixtures using an inverse regression approach, converges to the true one, under standard conditions. Finally, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds on the penalties hold true, in high-dimensional regression scenarios for a variety of MoE regression models, including Gaussian-gated and softmax-gated Gaussian MoE, based on an inverse regression strategy or a Lasso penalization, respectively. In particular, our oracle inequalities show that the performance in Jensen–Kullback–Leibler type loss of our penalized maximum likelihood estimators are roughly comparable to that of oracle models if we take large enough the constants in front of the penalties, whose forms are only known up to multiplicative constants and proportional to the dimensions of models. Such theoretical justifications of the penalty shapes motivate us to make use of the slope heuristic criterion to select several hyperparameters, including the number of mixture components, the amount of sparsity (the coefficients and ranks sparsity levels), the degree of polynomial mean functions, and the potential hidden block-diagonal structures of the covariance matrices of the multivariate predictor or response variable. To support our theoretical results and the statistical study of non-asymptotic model selection in a variety of MoE models, we perform numerical studies by considering simulated and real data, which highlight the performance of our finite-sample oracle inequality results.","tags":[],"title":"Model Selection and Approximation in High-dimensional Mixtures of Experts Models$:$ From Theory to Practice","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1635498000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635498000,"objectID":"e34718e1e3c53ee6c51ee0443f9503fd","permalink":"/talk/jedlehavre2021/","publishdate":"2021-09-24T10:30:00Z","relpermalink":"/talk/jedlehavre2021/","section":"talk","summary":"Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, bioinformatics, pattern recognition, economics, and medicine, among many others. They provide conditional constructions for regression in which the mixture weights, along with the component densities, are explained by the predictors, allowing for flexibility in the modeling of data arising from complex data generating processes. In this thesis, we study the approximation capabilities and model estimation and selection properties, of a wide variety of mixture distributions, with a particular focus on a rich family of MoE models in a high-dimensional setting, including MoE models with Gaussian experts and softmax or Gaussian gating functions, which are the most popular choices and are powerful tools for modeling complex non-linear relationships between responses and predictors that arise from different subpopulations. We consider both the theoretical statistical and methodological aspects, and the numerical tools, related to the conception of these models, as well as to their data-driven estimation and model selection. More precisely, in this thesis, we first review the universal approximation properties of classical mixture distributions in order to prepare the theoretical framework and to clarify some unclear and vague statements in the literature, before considering them in the context of MoE models. In particular, we prove that, to an arbitrary degree of accuracy, location-scale mixtures of a continuous probability density function (PDF) can approximate any continuous PDF, uniformly, on a compact set; and location-scale mixtures of an essentially-bounded PDF can approximate any PDF in Lebesgue spaces. Then, after improving upon approximation results in the context of unconditional mixture distributions, we study the universal approximation capabilities of MoE models in a variety of contexts, including conditional density approximation and approximate Bayesian computation (ABC). Given input and output variables are both compactly supported, we provide denseness results in Lebesgue spaces for conditional PDFs. Moreover, we prove that the quasi-posterior distribution resulting from ABC with surrogate posteriors built from finite Gaussian mixtures using an inverse regression approach, converges to the true one, under standard conditions. Finally, we establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds on the penalties hold true, in high-dimensional regression scenarios for a variety of MoE regression models, including Gaussian-gated and softmax-gated Gaussian MoE, based on an inverse regression strategy or a Lasso penalization, respectively. In particular, our oracle inequalities show that the performance in Jensen–Kullback–Leibler type loss of our penalized maximum likelihood estimators are roughly comparable to that of oracle models if we take large enough the constants in front of the penalties, whose forms are only known up to multiplicative constants and proportional to the dimensions of models. Such theoretical justifications of the penalty shapes motivate us to make use of the slope heuristic criterion to select several hyperparameters, including the number of mixture components, the amount of sparsity (the coefficients and ranks sparsity levels), the degree of polynomial mean functions, and the potential hidden block-diagonal structures of the covariance matrices of the multivariate predictor or response variable. To support our theoretical results and the statistical study of non-asymptotic model selection in a variety of MoE models, we perform numerical studies by considering simulated and real data, which highlight the performance of our finite-sample oracle inequality results.","tags":[],"title":"Model Selection and Approximation in High-dimensional Mixtures of Experts Models From Theory to Practice","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1632992400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632992400,"objectID":"9fdefc2d0450c058b517e4b36086b53f","permalink":"/talk/iaadm2021/","publishdate":"2021-09-14T10:30:00Z","relpermalink":"/talk/iaadm2021/","section":"talk","summary":"Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, bioinformatics, pattern recognition, economics, and medicine, among many others. They provide conditional constructions for regression in which the mixture weights, along with the component densities, are explained by the predictors,  allowing for flexibility in the modeling of data arising from complex data generating processes. In this work, we consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems both from the computational and theoretical points of view. We establish non-asymptotic risk bounds that take the form of weak oracle inequalities, provided that lower bounds on the penalties hold true, in high-dimensional regression scenarios for a variety of MoE regression models, including Gaussian-gated and softmax-gated Gaussian MoE, based on an inverse regression strategy or a Lasso penalization, respectively. In particular, our oracle inequalities show that the performance in Jensen–Kullback–Leibler type loss of our penalized maximum likelihood estimators are roughly comparable to that of oracle models if we take large enough the constants in front of the penalties, whose forms are only known up to multiplicative constants and proportional to the dimensions of models. Such theoretical justifications of the penalty shapes motivate us to make use of the slope heuristic criterion to select several hyperparameters, including the number of mixture components, the amount of sparsity (the coefficients and ranks sparsity levels), the degree of polynomial mean functions, and the potential hidden block-diagonal structures of the covariance matrices of the multivariate predictor or response variable. To support our theoretical results and the statistical study of non-asymptotic model selection in a variety of MoE models, we perform numerical studies by considering simulated and real data, which highlight the performance of our finite-sample oracle inequality results.","tags":[],"title":"Approximation and non-asymptotic model selection in mixture of experts models","type":"talk"},{"authors":["Hien Duy Nguyen","TrungTin Nguyen","Faicel Chamroukhi","Geoffrey J McLachlan"],"categories":null,"content":"","date":1628208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628208000,"objectID":"b3497d33d1bd2342c863ee13533e035e","permalink":"/publication/nguyen2020approximationmoe/","publishdate":"2021-08-06T00:00:00Z","relpermalink":"/publication/nguyen2020approximationmoe/","section":"publication","summary":"Mixture of experts (MoE) models are widely applied for conditional probability density estimation problems. We demonstrate the richness of the class of MoE models by proving denseness results in Lebesgue spaces, when inputs and outputs variables are both compactly supported. We further prove an almost uniform convergence result when the input is univariate. Auxiliary lemmas are proved regarding the richness of the soft-max gating function class, and their relationships to the class of Gaussian gating functions.","tags":["Source Themes"],"title":"Approximations of conditional probability density functions in Lebesgue spaces via mixture of experts models","type":"publication"},{"authors":["Julyan Arbel"],"categories":null,"content":"","date":1624838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624838400,"objectID":"a5639a7274e668b44cac6e5eb29a1386","permalink":"/talk/isba2021julyan/","publishdate":"2021-06-28T10:30:00Z","relpermalink":"/talk/isba2021julyan/","section":"talk","summary":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen and this can affect the approximations. Their choice is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics, to date. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated, a standard $L_2$ distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to functional summary statistics. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.","tags":[],"title":"Approximate Bayesian computation with surrogate posteriors","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1622639700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622639700,"objectID":"1dfbde0fa44b15cad559338a125b6c9e","permalink":"/talk/mhc2021/","publishdate":"2021-05-27T10:30:00Z","relpermalink":"/talk/mhc2021/","section":"talk","summary":"Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, due to its flexibility and effectiveness. We consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems, including feature selection, both from the computational and theoretical points of view. We study the problem of estimating the number of components of the GLoME model, in a penalized maximum likelihood estimation framework. We provide a lower bound on the penalty that ensures a weak oracle inequality is satisfied by our estimator. To support our theoretical result, we perform numerical experiments on simulated and real data, which highlight the performance of our finite-sample oracle inequality.","tags":[],"title":"A non-asymptotic model selection in mixture of experts models","type":"talk"},{"authors":["TrungTin Nguyen","Faicel Chamroukhi","Hien Duy Nguyen","Florence Forbes"],"categories":null,"content":"","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"602dc2840c77ccb8662864a1429d0b85","permalink":"/publication/nguyen2021nonblompe/","publishdate":"2021-04-18T00:00:00Z","relpermalink":"/publication/nguyen2021nonblompe/","section":"publication","summary":"Model selection via penalized likelihood type criteria is a standard task in many statistical inference and machine learning problems. It has led to deriving criteria with asymptotic consistency results and an increasing emphasis on introducing non-asymptotic criteria. We focus on the problem of modeling non-linear relationships in regression data with potential hidden graph-structured interactions between the high-dimensional predictors, within the mixture of experts modeling framework. In order to deal with such a complex situation, we investigate a block-diagonal localized mixture of polynomial experts (BLoMPE) regression model, which is constructed upon an inverse regression and block-diagonal structures of the Gaussian expert covariance matrices. We introduce a penalized maximum likelihood selection criterion to estimate the unknown conditional density of the regression model. This model selection criterion allows us to handle the challenging problem of inferring the number of mixture components, the degree of polynomial mean functions, and the hidden block-diagonal structures of the covariance matrices, which reduces the number of parameters to be estimated and leads to a trade-off between complexity and sparsity in the model. In particular, we provide a strong theoretical guarantee$:$ a finite-sample oracle inequality satisfied by the penalized maximum likelihood estimator with a Jensen-Kullback-Leibler type loss, to support the introduced non-asymptotic model selection criterion. The penalty shape of this criterion depends on the complexity of the considered random subcollection of BLoMPE models, including the relevant graph structures, the degree of polynomial mean functions, and the number of mixture components.","tags":["Source Themes"],"title":"A non-asymptotic model selection in block-diagonal mixture of polynomial experts models","type":"publication"},{"authors":["Florence Forbes"],"categories":null,"content":"","date":1618207200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618207200,"objectID":"41ac35a301ca3223330dad13ad20bcad","permalink":"/talk/abcsvalbard2021florence/","publishdate":"2020-04-12T10:30:00Z","relpermalink":"/talk/abcsvalbard2021florence/","section":"talk","summary":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen and this can affect the approximations. Their choice is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics, to date. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated, a standard $L_2$ distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to functional summary statistics. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.","tags":[],"title":"Approximate Bayesian computation with surrogate posteriors","type":"talk"},{"authors":["Hien Duy Nguyen"],"categories":null,"content":"","date":1618207200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618207200,"objectID":"5aff3f679382a2dfd391c45af4df13c7","permalink":"/talk/abcsvalbard2021hien/","publishdate":"2020-04-12T10:30:00Z","relpermalink":"/talk/abcsvalbard2021hien/","section":"talk","summary":"Approximate Bayesian computation (ABC) has become an essential part of the Bayesian toolbox for addressing problems in which the likelihood is prohibitively expensive or entirely unknown, making it intractable. ABC defines a pseudo-posterior by comparing observed data with simulated data, traditionally based on some summary statistics, the elicitation of which is regarded as a key difficulty. Recently, using data discrepancy measures has been proposed in order to bypass the construction of summary statistics. Here we propose to use the importance-sampling ABC (IS-ABC) algorithm relying on the so-called two-sample energy statistic. We establish a new asymptotic result for the case where both the observed sample size and the simulated data sample size increase to infinity, which highlights to what extent the data discrepancy measure impacts the asymptotic pseudo-posterior. The result holds in the broad setting of IS-ABC methodologies, thus generalizing previous results that have been established only for rejection ABC algorithms. Furthermore, we propose a consistent V-statistic estimator of the energy statistic, under which we show that the large sample result holds, and prove that the rejection ABC algorithm, based on the energy statistic, generates pseudo-posterior distributions that achieves convergence to the correct limits, when implemented with rejection thresholds that converge to zero, in the finite sample setting. Our proposed energy statistic based ABC algorithm is demonstrated on a variety of models, including a Gaussian mixture, a moving-average model of order two, a bivariate beta and a multivariate g-and-k distribution. We find that our proposed method compares well with alternative discrepancy measures.","tags":[],"title":"Distance-based ABC procedures","type":"talk"},{"authors":["TrungTin Nguyen"],"categories":null,"content":"","date":1617872400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617872400,"objectID":"db325529a7e7879cf551841b0ac5d052","permalink":"/talk/mimo2021/","publishdate":"2021-04-08T09:00:00Z","relpermalink":"/talk/mimo2021/","section":"talk","summary":"Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, due to its flexibility and effectiveness. We consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems, including feature selection, both from the computational and theoretical points of view. We study the problem of selecting of the GLoME model characterized by the number of components, in a penalized maximum likelihood estimation framework. We provide a lower bound on the penalty that ensures a weak oracle inequality is satisfied by our estimator. To support our theoretical result, we perform numerical experiments on simulated and real data, which illustrate the performance of our finite-sample oracle inequality.","tags":[],"title":"Non-asymptotic penalization criteria for model selection in mixture of experts models","type":"talk"},{"authors":["Florence Forbes"],"categories":null,"content":"","date":1614186000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614186000,"objectID":"a62e3213958f3c1cc6886116f064e767","permalink":"/talk/laplacedemon2021/","publishdate":"2020-02-24T17:00:00Z","relpermalink":"/talk/laplacedemon2021/","section":"talk","summary":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancy and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen and this can affect the approximations. Their choice is an active research topic which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics, to date. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated, a standard $L_2$ distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to functional summary statistics. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.","tags":[],"title":"Approximate Bayesian computation with surrogate posteriors","type":"talk"},{"authors":null,"categories":null,"content":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen and this can affect the approximations. Their choice is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics, to date. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated, a standard L2 distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to functional summary statistics. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.\n","date":1613088000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613088000,"objectID":"6f23c4066938deecf6d308178ba55a6d","permalink":"/project/simulation-based-inference/","publishdate":"2021-02-12T00:00:00Z","relpermalink":"/project/simulation-based-inference/","section":"project","summary":"A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly.","tags":["Simulation-based Inference"],"title":"Simulation-based Inference","type":"project"},{"authors":["TrungTin Nguyen","Hien Duy Nguyen","Faicel Chamroukhi","Geoffrey J McLachlan"],"categories":null,"content":"","date":1600732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600732800,"objectID":"d474312c8baf1d786573b3df429d6d19","permalink":"/publication/nguyen2020l1oracle/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/publication/nguyen2020l1oracle/","section":"publication","summary":"Mixture-of-experts (MoE) models are a popular framework for modeling heterogeneity in data, for both regression and classification problems in statistics and machine learning, due to their flexibility and the abundance of statistical estimation and model choice tools. Such flexibility comes from allowing the mixture weights (or gating functions) in the MoE model to depend on the explanatory variables, along with the experts (or component densities). This permits the modeling of data arising from more complex data generating processes, compared to the classical finite mixtures and finite mixtures of regression models, whose mixing parameters are independent of the covariates. The use of MoE models in a high-dimensional setting, when the number of explanatory variables can be much larger than the sample size (i.e., $p \\gg n)$, is challenging from a computational point of view, and in particular from a theoretical point of view, where the literature is still lacking results in dealing with the curse of dimensionality, in both the statistical estimation and feature selection. We consider the finite mixture-of-experts model with soft-max gating functions and Gaussian experts for high-dimensional regression on heterogeneous data, and its $l_1$-regularized estimation via the Lasso. We focus on the Lasso estimation properties rather than its feature selection properties. We provide a lower bound on the regularization parameter of the Lasso function that ensures an $l_1$-oracle inequality satisfied by the Lasso estimator according to the Kullback-Leibler loss.","tags":["Source Themes"],"title":"An l1-oracle inequality for the Lasso in mixture-of-experts regression models","type":"publication"},{"authors":null,"categories":null,"content":"","date":1600732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600732800,"objectID":"cdb02e883a88dc2891d64fb28cdc5984","permalink":"/project/model-selection-in-mixture-of-experts-models/","publishdate":"2020-09-22T00:00:00Z","relpermalink":"/project/model-selection-in-mixture-of-experts-models/","section":"project","summary":"Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, due to its flexibility and effectiveness. We consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems, including feature selection, both from the computational and theoretical points of view. We study the problem of estimating the number of components of the GLoME model, in a penalized maximum likelihood estimation framework. We provide a lower bound on the penalty that ensures a weak oracle inequality is satisfied by our estimator. To support our theoretical result, we perform numerical experiments on simulated and real data, which illustrate the performance of our finite-sample oracle inequality.","tags":["Model selection in mixture of experts models"],"title":"Model Selection in Mixture of Experts Models","type":"project"},{"authors":["TrungTin Nguyen","Hien Duy Nguyen","Faicel Chamroukhi","Geoffrey J McLachlan"],"categories":null,"content":"","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"f5e4d281368cc4fda255d6f9eefd4afb","permalink":"/publication/nguyen2020approximation/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/publication/nguyen2020approximation/","section":"publication","summary":"Given sufficiently many components, it is often cited that finite mixture models can approximate any other probability density function (PDF) to an arbitrary degree of accuracy. Unfortunately, the nature of this approximation result is often left unclear. We prove that finite mixture models constructed from pdfs in $C_0$ can be used to conduct approximation of various classes of approximands in a number of different modes. That is, we prove approximands in C0 can be uniformly approximated, approximands in $C_b$ can be uniformly approximated on compact sets, and approximands in Lp can be approximated with respect to the $L_p$, for $p\\in [1,\\infty)$. Furthermore, we also prove that measurable functions can be approximated, almost everywhere.","tags":["Source Themes"],"title":"Approximation by finite mixtures of continuous density functions that vanish at infinity","type":"publication"},{"authors":null,"categories":null,"content":"","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"bb935adb8b1c2fddfae5a999b0947d02","permalink":"/project/approximation-capabilities-of-the-mixture-of-experts-models/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/project/approximation-capabilities-of-the-mixture-of-experts-models/","section":"project","summary":"Finite mixture models and mixture of experts models are an expansive and expressive class of probability models that have been successfully applied in many situations where data follow a complex generative process that may be highly heterogeneous. It has long been known that finite mixture models, under sufficient regularity conditions, can approximate any probability density functions to arbitrary degrees of accuracy, and such results have been established under varying assumptive restrictions. Our work seeks to provide the weakest set of assumptions in order to establish approximation theoretic results over the widest class of probability density problems, possible. In particular, we demonstrate the richness of the class of MoE models by proving denseness results in Lebesgue spaces, when inputs and outputs variables are both compactly supported. We further prove an almost uniform convergence result when the input is univariate. Auxiliary lemmas are proved regarding the richness of the soft-max gating function class, and their relationships to the class of Gaussian gating functions. The result provides further evidence towards the success of mixture models in applications, and provides mathematical guarantees to practitioners who apply finite mixture models as well as mixture of experts models in their analytic problems.","tags":["Approximation capabilities of the mixture of experts models"],"title":"Approximation Capabilities of Mixture of Experts Models","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"\u0026mdash; commentable: false date: \u0026ldquo;2018-06-28T00:00:00+01:00\u0026rdquo; draft: true editable: false header: caption: \u0026quot;\u0026quot; image: \u0026quot;\u0026quot; share: false title: Privacy Policy \u0026mdash; \u0026hellip; ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/privacy/","section":"","summary":"\u0026mdash; commentable: false date: \u0026ldquo;2018-06-28T00:00:00+01:00\u0026rdquo; draft: true editable: false header: caption: \u0026quot;\u0026quot; image: \u0026quot;\u0026quot; share: false title: Privacy Policy \u0026mdash; \u0026hellip; ","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"\u0026mdash; commentable: false date: \u0026ldquo;2018-06-28T00:00:00+01:00\u0026rdquo; draft: true editable: false header: caption: \u0026quot;\u0026quot; image: \u0026quot;\u0026quot; share: false title: Terms \u0026mdash; \u0026hellip; ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"/terms/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/terms/","section":"","summary":"\u0026mdash; commentable: false date: \u0026ldquo;2018-06-28T00:00:00+01:00\u0026rdquo; draft: true editable: false header: caption: \u0026quot;\u0026quot; image: \u0026quot;\u0026quot; share: false title: Terms \u0026mdash; \u0026hellip; ","tags":null,"title":"","type":"page"}]