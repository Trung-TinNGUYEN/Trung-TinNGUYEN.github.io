<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="TrungTin Nguyen">

  
  
  
    
  
  <meta name="description" content="Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, due to its flexibility and effectiveness. We consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems, including feature selection, both from the computational and theoretical points of view. We study the problem of estimating the number of components of the GLoME model, in a penalized maximum likelihood estimation framework. We provide a lower bound on the penalty that ensures a weak oracle inequality is satisfied by our estimator. To support our theoretical result, we perform numerical experiments on simulated and real data, which illustrate the performance of our finite-sample oracle inequality.">

  
  <link rel="alternate" hreflang="en-us" href="/project/model-selection-in-mixture-of-experts-models/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-170614851-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-170614851-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huf3d19fa03233384ac87f6266a08cd3fc_74825_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huf3d19fa03233384ac87f6266a08cd3fc_74825_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="/project/model-selection-in-mixture-of-experts-models/">

  
  
  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@TrungTinNguyen0">
  <meta property="twitter:creator" content="@TrungTinNguyen0">
  
  <meta property="og:site_name" content="TrungTin Nguyen">
  <meta property="og:url" content="/project/model-selection-in-mixture-of-experts-models/">
  <meta property="og:title" content="Model Selection in Mixture of Experts Models | TrungTin Nguyen">
  <meta property="og:description" content="Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, due to its flexibility and effectiveness. We consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems, including feature selection, both from the computational and theoretical points of view. We study the problem of estimating the number of components of the GLoME model, in a penalized maximum likelihood estimation framework. We provide a lower bound on the penalty that ensures a weak oracle inequality is satisfied by our estimator. To support our theoretical result, we perform numerical experiments on simulated and real data, which illustrate the performance of our finite-sample oracle inequality."><meta property="og:image" content="/project/model-selection-in-mixture-of-experts-models/featured.png">
  <meta property="twitter:image" content="/project/model-selection-in-mixture-of-experts-models/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-09-22T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-09-22T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/project/model-selection-in-mixture-of-experts-models/"
  },
  "headline": "Model Selection in Mixture of Experts Models",
  
  "image": [
    "/project/model-selection-in-mixture-of-experts-models/featured.png"
  ],
  
  "datePublished": "2020-09-22T00:00:00Z",
  "dateModified": "2020-09-22T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "TrungTin Nguyen"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "TrungTin Nguyen",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_huf3d19fa03233384ac87f6266a08cd3fc_74825_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, due to its flexibility and effectiveness. We consider the Gaussian-gated localized MoE (GLoME) regression model for modeling heterogeneous data. This model poses challenging questions with respect to the statistical estimation and model selection problems, including feature selection, both from the computational and theoretical points of view. We study the problem of estimating the number of components of the GLoME model, in a penalized maximum likelihood estimation framework. We provide a lower bound on the penalty that ensures a weak oracle inequality is satisfied by our estimator. To support our theoretical result, we perform numerical experiments on simulated and real data, which illustrate the performance of our finite-sample oracle inequality."
}
</script>

  

  


  


  





  <title>Model Selection in Mixture of Experts Models | TrungTin Nguyen</title>

</head>
<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  









<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">TrungTin Nguyen</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">TrungTin Nguyen</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/CV_TrungTin_Nguyen.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article article-project">

  




















  
  


<div class="article-container pt-3">
  <h1>Model Selection in Mixture of Experts Models</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 22, 2020
  </span>
  

  

  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 700px; max-height: 1050px;">
  <div style="position: relative">
    <img src="/project/model-selection-in-mixture-of-experts-models/featured.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      
    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/model-selection-in-mixture-of-experts-models/">Model selection in mixture of experts models</a>
  
</div>














  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/trungtin-nguyen/avatar_hued61db7a7fd767a00e76edbe4a3b2c83_1146978_270x270_fill_q90_lanczos_center.jpeg" alt="TrungTin Nguyen">
      

      <div class="media-body">
        <h5 class="card-title"><a href="/">TrungTin Nguyen</a></h5>
        <h6 class="card-subtitle">Postdoctoral Fellow</h6>
        <p class="card-text">A central theme of my research is data science at the intersection of statistical learning, machine learning and optimization.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=NhiJDJsAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.researchgate.net/profile/Trungtin-Nguyen-2" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.semanticscholar.org/author/1899270520" target="_blank" rel="noopener">
        <i class="ai ai-semantic-scholar"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/0000-0001-8433-5980" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://dblp.org/pid/275/3643.html" target="_blank" rel="noopener">
        <i class="ai ai-dblp"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Trung-TinNGUYEN" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:trungtinnguyends@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/TrungTinNguyen0" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/trungtinnguyen0/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.instagram.com/trung_tin__nguyen/" target="_blank" rel="noopener">
        <i class="fab fa-instagram"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  










<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/project/simulation-based-inference/" rel="next">Simulation-based Inference</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/project/approximation-capabilities-of-the-mixture-of-experts-models/" rel="prev">Approximation Capabilities of Mixture of Experts Models</a>
  </div>
  
</div>

</div>





  
  





    <div class="project-related-pages content-widget-hr">
      
      

      
      
      

      
      
      
        <h2>Publications</h2>
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2023bayesian/" >Bayesian nonparametric mixture of experts for high-dimensional inverse problems</a>
    </h3>

    
    <a href="/publication/nguyen2023bayesian/" class="summary-link">
      <div class="article-style">
        A large class of problems can be formulated as inverse problems, where the goal is to find parameter values that best explain some …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>, <span >Florence Forbes</span>, <span >Julyan Arbel</span>, <span >Hien Duy Nguyen</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://hal.science/hal-04015203" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2023bayesian/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2023nonasymptotic/" >A non-asymptotic risk bound for model selection in high-dimensional mixture of experts via joint rank and variable selection</a>
    </h3>

    
    <a href="/publication/nguyen2023nonasymptotic/" class="summary-link">
      <div class="article-style">
        We are motivated by the problem of identifying potentially nonlinear regression relationships between high-dimensional outputs and …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>, <span >Dung Ngoc Nguyen</span>, <span >Hien Duy Nguyen</span>, <span >Faicel Chamroukhi</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://hal.science/hal-03984011" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2023nonasymptotic/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2022nonasymptotic/" >A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts</a>
    </h3>

    
    <a href="/publication/nguyen2022nonasymptotic/" class="summary-link">
      <div class="article-style">
        Mixture of experts (MoE) are a popular class of statistical and machine learning models that have gained attention over the years due …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>, <span >Hien Duy Nguyen</span>, <span >Faicel Chamroukhi</span>, <span >Florence Forbes</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1214/22-EJS2057" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2022nonasymptotic/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1214/22-EJS2057" target="_blank" rel="noopener">
  DOI
</a>



    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2022model/" >Model selection by penalization in mixture of experts models with a non-asymptotic approach</a>
    </h3>

    
    <a href="/publication/nguyen2022model/" class="summary-link">
      <div class="article-style">
        This study is devoted to the problem of model selection among a collection of Gaussian-gated localized mixtures of experts models …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>, <span >Faicel Chamroukhi</span>, <span >Hien Duy Nguyen</span>, <span >Florence Forbes</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://hal.archives-ouvertes.fr/hal-03663348" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2022model/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2021modelthesis/" >Model Selection and Approximation in High-dimensional Mixtures of Experts Models: from Theory to Practice</a>
    </h3>

    
    <a href="/publication/nguyen2021modelthesis/" class="summary-link">
      <div class="article-style">
        Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://tel.archives-ouvertes.fr/tel-03524749/" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2021modelthesis/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/Trung-TinNGUYEN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://trung-tinnguyen.github.io/talks/Presentation_Soutenance_These_TrungTinNguyen.pdf" target="_blank" rel="noopener">
  Slides
</a>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://www.youtube.com/watch?v=aLwMzQkL4UU" target="_blank" rel="noopener">
  Video
</a>





    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2021nonblompe/" >A non-asymptotic model selection in block-diagonal mixture of polynomial experts models</a>
    </h3>

    
    <a href="/publication/nguyen2021nonblompe/" class="summary-link">
      <div class="article-style">
        Model selection via penalized likelihood type criteria is a standard task in many statistical inference and machine learning problems. It has led to deriving criteria with asymptotic consistency results and an increasing emphasis on introducing non-asymptotic criteria. We focus on the problem of modeling non-linear relationships in regression data with potential hidden graph-structured interactions between the high-dimensional predictors, within the mixture of experts modeling framework. In order to deal with such a complex situation, we investigate a block-diagonal localized mixture of polynomial experts (BLoMPE) regression model, which is constructed upon an inverse regression and block-diagonal structures of the Gaussian expert covariance matrices. We introduce a penalized maximum likelihood selection criterion to estimate the unknown conditional density of the regression model. This model selection criterion allows us to handle the challenging problem of inferring the number of mixture components, the degree of polynomial mean functions, and the hidden block-diagonal structures of the covariance matrices, which reduces the number of parameters to be estimated and leads to a trade-off between complexity and sparsity in the model. In particular, we provide a strong theoretical guarantee$:$ a finite-sample oracle inequality satisfied by the penalized maximum likelihood estimator with a Jensen-Kullback-Leibler type loss, to support the introduced non-asymptotic model selection criterion. The penalty shape of this criterion depends on the complexity of the considered random subcollection of BLoMPE models, including the relevant graph structures, the degree of polynomial mean functions, and the number of mixture components.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>, <span >Faicel Chamroukhi</span>, <span >Hien Duy Nguyen</span>, <span >Florence Forbes</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/pdf/2104.08959.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2021nonblompe/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/nguyen2020l1oracle/" >An l1-oracle inequality for the Lasso in mixture-of-experts regression models</a>
    </h3>

    
    <a href="/publication/nguyen2020l1oracle/" class="summary-link">
      <div class="article-style">
        Mixture-of-experts (MoE) models are a popular framework for modeling heterogeneity in data, for both regression and classification problems in statistics and machine learning, due to their flexibility and the abundance of statistical estimation and model choice tools. Such flexibility comes from allowing the mixture weights (or gating functions) in the MoE model to depend on the explanatory variables, along with the experts (or component densities). This permits the modeling of data arising from more complex data generating processes, compared to the classical finite mixtures and finite mixtures of regression models, whose mixing parameters are independent of the covariates. The use of MoE models in a high-dimensional setting, when the number of explanatory variables can be much larger than the sample size (i.e., $p \gg n)$, is challenging from a computational point of view, and in particular from a theoretical point of view, where the literature is still lacking results in dealing with the curse of dimensionality, in both the statistical estimation and feature selection. We consider the finite mixture-of-experts model with soft-max gating functions and Gaussian experts for high-dimensional regression on heterogeneous data, and its $l_1$-regularized estimation via the Lasso. We focus on the Lasso estimation properties rather than its feature selection properties. We provide a lower bound on the regularization parameter of the Lasso function that ensures an $l_1$-oracle inequality satisfied by the Lasso estimator according to the Kullback-Leibler loss.
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>, <span >Hien Duy Nguyen</span>, <span >Faicel Chamroukhi</span>, <span >Geoffrey J McLachlan</span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2009.10622" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/nguyen2020l1oracle/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

          
        
      

      
      
      
        <h2>Talks</h2>
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/jds2022/" >Model selection by penalization in mixture of experts models with a non-asymptotic approach.</a>
    </h3>

    
    <a href="/talk/jds2022/" class="summary-link">
      <div class="article-style">
        This study is devoted to the problem of model selection among a collection of Gaussian-gated localized mixtures of experts models …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Jun 13, 2022 2:00 PM &amp;mdash; Jun 17, 2022 4:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Université Claude Bernard Lyon 1, France</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNguyen0" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/jds2022/" >
      <img src="/talk/jds2022/featured_hue34f3e46766702aa7e3c8eed1c6e4da7_23599_150x0_resize_q90_lanczos.jpg" alt="Model selection by penalization in mixture of experts models with a non-asymptotic approach.">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/sasviasm2022/" >A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts models.</a>
    </h3>

    
    <a href="/talk/sasviasm2022/" class="summary-link">
      <div class="article-style">
        Mixture of experts (MoE) are a popular class of statistical and machine learning models that have gained attention over the years due …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          May 11, 2022 2:00 PM &amp;mdash; 4:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Vietnam Institute for Advanced Study in Mathematics, Vietnam</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNguyen0" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/sasviasm2022/" >
      <img src="/talk/sasviasm2022/featured_hu7fba9da1679ce4a56c592454604cb9c1_388413_150x0_resize_q90_lanczos.jpg" alt="A non-asymptotic approach for model selection via penalization in high-dimensional mixture of experts models.">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/statlearn2022/" >A non-asymptotic approach for model selection via penalization in mixture of experts models</a>
    </h3>

    
    <a href="/talk/statlearn2022/" class="summary-link">
      <div class="article-style">
        Mixture of experts (MoE), originally introduced as a neural network, is a popular class of statistical and machine learning models that …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Apr 4, 2022 9:00 AM &amp;mdash; Apr 8, 2022 6:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Institut d&#39;Etudes Scientifiques de Cargèse, Corsica</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="ttps://twitter.com/TrungTinNguyen0" >
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/statlearn2022/" >
      <img src="/talk/statlearn2022/featured_hu2f5bdca85041d4c9e50538dff9a0e4b4_1818630_150x0_resize_q90_lanczos.jpg" alt="A non-asymptotic approach for model selection via penalization in mixture of experts models">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/ensai2022/" >A non-asymptotic model selection in mixture of experts models</a>
    </h3>

    
    <a href="/talk/ensai2022/" class="summary-link">
      <div class="article-style">
        Mixture of experts (MoE), originally introduced as a neural network, is a popular class of statistical and machine learning models that …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Mar 18, 2022 11:00 AM &amp;mdash; 12:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>ENSAI École Nationale de Statistique et Analyse de l&#39;Information, France</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNGUYE10" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/ensai2022/" >
      <img src="/talk/ensai2022/featured_hu92456580e56fddf4f3c592621d13c105_114095_150x0_resize_q90_lanczos.jpg" alt="A non-asymptotic model selection in mixture of experts models">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/soutenancedethese2021decembre14/" >Model Selection and Approximation in High-dimensional Mixtures of Experts Models$:$ From Theory to Practice</a>
    </h3>

    
    <a href="/talk/soutenancedethese2021decembre14/" class="summary-link">
      <div class="article-style">
        Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Dec 14, 2021 1:30 PM &amp;mdash; 6:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Laboratoire de Mathématiques Nicolas Oresme, Université de Caen Normandie, France</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://trung-tinnguyen.github.io/files/PhDThesis2021_Manuscript_TrungTin_Nguyen.pdf" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/Trung-TinNGUYEN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  

  
    
  

  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/approximation-capabilities-of-the-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNGUYE10" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/soutenancedethese2021decembre14/" >
      <img src="/talk/soutenancedethese2021decembre14/featured_hu471ec6234627cb33b6a4ac8181e1871f_47772_150x0_resize_q90_lanczos.jpg" alt="Model Selection and Approximation in High-dimensional Mixtures of Experts Models$:$ From Theory to Practice">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/jedlehavre2021/" >Model Selection and Approximation in High-dimensional Mixtures of Experts Models From Theory to Practice</a>
    </h3>

    
    <a href="/talk/jedlehavre2021/" class="summary-link">
      <div class="article-style">
        Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Oct 29, 2021 9:00 AM &amp;mdash; 6:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Laboratoire d&#39;Ondes et Milieux Complexes, Le Havre, France</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNGUYE10" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/jedlehavre2021/" >
      <img src="/talk/jedlehavre2021/featured_hu8fc3d5e88e511b356ad8408cfd0c3eb5_37639_150x0_resize_q90_lanczos.jpg" alt="Model Selection and Approximation in High-dimensional Mixtures of Experts Models From Theory to Practice">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/iaadm2021/" >Approximation and non-asymptotic model selection in mixture of experts models</a>
    </h3>

    
    <a href="/talk/iaadm2021/" class="summary-link">
      <div class="article-style">
        Mixtures of experts (MoE) models are a ubiquitous tool for the analysis of heterogeneous data across many fields including statistics, …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Sep 30, 2021 9:00 AM &amp;mdash; Sep 29, 2021 5:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>INSA Rouen Normandie, Rouen, France</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNGUYE10" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/iaadm2021/" >
      <img src="/talk/iaadm2021/featured_hu92a67e76221557ac83afa85ce489f42c_2576382_150x0_resize_q90_lanczos.jpg" alt="Approximation and non-asymptotic model selection in mixture of experts models">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/mhc2021/" >A non-asymptotic model selection in mixture of experts models</a>
    </h3>

    
    <a href="/talk/mhc2021/" class="summary-link">
      <div class="article-style">
        Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Jun 2, 2021 &amp;mdash; Jun 4, 2021
        </span>
        
        <span class="middot-divider"></span>
        <span>Institut de Mathématique d&#39;Orsay, Paris, France</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNGUYE10" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/mhc2021/" >
      <img src="/talk/mhc2021/featured_hu6405b1b2e04e55377091f78f9fff669a_2539722_150x0_resize_q90_lanczos.jpg" alt="A non-asymptotic model selection in mixture of experts models">
    </a>
    
  </div>
</div>

          
        
          
            








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/talk/mimo2021/" >Non-asymptotic penalization criteria for model selection in mixture of experts models</a>
    </h3>

    
    <a href="/talk/mimo2021/" class="summary-link">
      <div class="article-style">
        Mixture of experts (MoE) is a popular class of models in statistics and machine learning that has sustained attention over the years, …
      </div>
    </a>
    

    <div class="stream-meta article-metadata">

      
      <div>
        <span>
          Apr 8, 2021 9:00 AM &amp;mdash; Apr 9, 2021 3:00 PM
        </span>
        
        <span class="middot-divider"></span>
        <span>Laboratory of Mathematics Raphaël Salem (LMRS, UMR CNRS 6085)</span>
        
      </div>
      

      
      <div>
        

  
  <span class="font-weight-bold"><a href="/author/trungtin-nguyen/">TrungTin Nguyen</a></span>
      </div>
      
    </div>

    
    <div class="btn-links">
      








  









  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/model-selection-in-mixture-of-experts-models/">
    Project
  </a>
  










  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://twitter.com/TrungTinNGUYE10" target="_blank" rel="noopener">
    <i class="fab fa-twitter mr-1"></i>
    Follow
  </a>


    </div>
    

  </div>
  <div class="ml-3">
    
    
    
    <a href="/talk/mimo2021/" >
      <img src="/talk/mimo2021/featured_hu8e302b6e71c490986e2e55bbbf98fafe_87394_150x0_resize_q90_lanczos.jpg" alt="Non-asymptotic penalization criteria for model selection in mixture of experts models">
    </a>
    
  </div>
</div>

          
        
      
    </div>
  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.23966d19f6915d56260b05a4b1937395.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/"></a>
    
    
       &middot; 
      <a href="/terms/"></a>
    
  </p>
  

  <p class="powered-by">
    © 2024 TrungTin Nguyen
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
