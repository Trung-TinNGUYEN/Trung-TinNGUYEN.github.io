<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | TrungTin Nguyen</title>
    <link>/project/</link>
      <atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2023 TrungTin Nguyen</copyright><lastBuildDate>Sat, 01 Apr 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/avatar2.jpg</url>
      <title>Projects</title>
      <link>/project/</link>
    </image>
    
    <item>
      <title>Convergence Rates for Mixtures of Experts</title>
      <link>/project/convergence-rates-for-mixtures-of-experts/</link>
      <pubDate>Sat, 01 Apr 2023 00:00:00 +0000</pubDate>
      <guid>/project/convergence-rates-for-mixtures-of-experts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Neural Networks</title>
      <link>/project/deep-neural-networks/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>/project/deep-neural-networks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bayesian Nonparametrics in Mixture of Experts Models</title>
      <link>/project/bayesian-nonparametrics-in-mixture-of-experts-models/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>/project/bayesian-nonparametrics-in-mixture-of-experts-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simulation-based Inference</title>
      <link>/project/simulation-based-inference/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/project/simulation-based-inference/</guid>
      <description>&lt;p&gt;A key ingredient in approximate Bayesian computation (ABC) procedures is the choice of a discrepancy that describes how different the simulated and observed data are, often based on a set of summary statistics when the data cannot be compared directly. Unless discrepancies and summaries are available from experts or prior knowledge, which seldom occurs, they have to be chosen and this can affect the approximations. Their choice is an active research topic, which has mainly considered data discrepancies requiring samples of observations or distances between summary statistics, to date. In this work, we introduce a preliminary learning step in which surrogate posteriors are built from finite Gaussian mixtures using an inverse regression approach. These surrogate posteriors are then used in place of summary statistics and compared using metrics between distributions in place of data discrepancies. Two such metrics are investigated, a standard L2 distance and an optimal transport-based distance. The whole procedure can be seen as an extension of the semi-automatic ABC framework to functional summary statistics. The resulting ABC quasi-posterior distribution is shown to converge to the true one, under standard conditions. Performance is illustrated on both synthetic and real data sets, where it is shown that our approach is particularly useful when the posterior is multimodal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Model Selection in Mixture of Experts Models</title>
      <link>/project/model-selection-in-mixture-of-experts-models/</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/project/model-selection-in-mixture-of-experts-models/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Approximation Capabilities of Mixture of Experts Models</title>
      <link>/project/approximation-capabilities-of-the-mixture-of-experts-models/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/project/approximation-capabilities-of-the-mixture-of-experts-models/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
